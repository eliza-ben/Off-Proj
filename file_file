"""
File-to-File ETL Service - Production-Ready File Format Conversion Microservice

This service converts files between different formats with:
- Full validation (Avro schemas)
- Data transformation (JOLT)
- Format conversion (CSV â†” JSON â†” JSONL â†” Fixed-Width)
- Compression support
- Audit trail via Kafka

Service Port: 9093
Base URL: http://localhost:9093
Framework: FastAPI + AsyncIO
Language: Python 3.11+

Key Features:
-------------
1. File Reading
   - CSV/TSV: Delimited files with headers
   - JSON: Single JSON object or array
   - JSONL: JSON Lines (newline-delimited JSON)
   - Fixed-Width: Positional data files
   - EDI: Electronic Data Exchange files
   - Custom encoding support

2. File Writing
   - CSV/TSV: Delimited output
   - JSON: Pretty or compact
   - JSONL: Streaming JSON Lines
   - Fixed-Width: Formatted positional output
   - Gzip compression

3. Format Conversion Matrix
   - CSV â†’ JSON, JSONL, TSV, Fixed-Width
   - JSON â†’ CSV, JSONL, TSV, Fixed-Width
   - JSONL â†’ CSV, JSON, TSV, Fixed-Width
   - Fixed-Width â†’ CSV, JSON, JSONL, TSV
   - EDI â†’ CSV, JSON, JSONL, TSV, Fixed-Width
   - All formats support bidirectional conversion

4. Data Transformation
   - JOLT transformation integration
   - Avro schema validation (source + target)
   - Field mapping with defaults
   - Type conversion

5. Audit & Observability
   - Kafka audit events
   - Structured JSON logging
   - Prometheus metrics
   - Correlation ID tracking

Endpoints:
----------
- POST /convert - Main conversion endpoint
- GET /health - Health check
- GET /metrics - Prometheus metrics

For detailed API documentation, see FILE_TO_FILE_SPECIFICATION.md
"""

import os
import time
from typing import Any, Dict, List, Optional
from datetime import datetime
from pathlib import Path
from fastapi import FastAPI, HTTPException, Depends, Request, Header
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# Import from shared library
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../../libs/py-common"))

# Load environment variables from local .env (if present)
load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env", override=False)

from common.auth import verify_jwt_or_basic
from common.jolt import JoltClient
from common.metrics import mount_metrics, OPERATION_LATENCY, record_success, record_error, record_batch
from common.processors import ETLPipeline, FileReader, FileWriter
from common.aggregation import AggregationConfig
from common.logging import setup_logging, log_with_context, log_error, get_correlation_id
from common.middleware import add_correlation_middleware
from common.audit import (
    setup_audit,
    publish_audit_event,
)
from common.audit_middleware import add_audit_middleware
from common.security import (
    add_cors_middleware,
    create_limiter,
    add_rate_limiting,
    add_request_size_limit,
    STRICT_RATE_LIMIT,
)


SERVICE_NAME = "file-to-file"
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")
JSON_LOGS = os.getenv("JSON_LOGS", "false").lower() == "true"
AUDIT_ENABLED = os.getenv("AUDIT_ENABLED", "true").lower() == "true"
DEFAULT_DOMAIN_ID = os.getenv("DEFAULT_DOMAIN_ID", "default")

# Setup structured logging
logger = setup_logging(SERVICE_NAME, LOG_LEVEL, JSON_LOGS)

# Setup audit publisher
audit_publisher = setup_audit(
    SERVICE_NAME,
    default_domain_id=DEFAULT_DOMAIN_ID,
    kafka_bootstrap_servers=os.getenv("KAFKA_BOOTSTRAP_SERVERS"),
    kafka_audit_topic=os.getenv("KAFKA_AUDIT_TOPIC"),
    enabled=AUDIT_ENABLED,
)

app = FastAPI(
    title="File-to-File Conversion Service",
    description="Convert files between formats with Avro validation and JOLT transformation",
    version="1.0.0",
)

# ============================================================================
# SECURITY MIDDLEWARE (Order Matters - add security first)
# ============================================================================

# 1. Request size limit (check before processing)
add_request_size_limit(app, max_size=10485760)  # 10MB limit

# 2. CORS protection
add_cors_middleware(app)

# 3. Rate limiting
limiter = create_limiter()
add_rate_limiting(app, limiter)

# ============================================================================
# OBSERVABILITY MIDDLEWARE
# ============================================================================

# Mount metrics
mount_metrics(app)

# Add correlation ID middleware
add_correlation_middleware(app, logger)

# Add audit middleware
add_audit_middleware(app, SERVICE_NAME, default_domain_id=DEFAULT_DOMAIN_ID)


# ============================================================================
# REQUEST/RESPONSE MODELS
# ============================================================================

class SourceFileConfig(BaseModel):
    """
    Source file configuration for reading.
    
    Supports multiple file formats:
    - CSV/TSV: Delimited files with optional headers
    - JSON: Single JSON object or array (with JSONPath extraction)
    - JSONL: JSON Lines (newline-delimited JSON)
    - Fixed-Width: Positional data with field widths
    
    Examples:
    ---------
    CSV file with header:
        {
            "path": "/data/users.csv",
            "type": "csv",
            "has_header": true,
            "encoding": "utf-8"
        }
    
    JSON file with nested data:
        {
            "path": "/data/response.json",
            "type": "json",
            "records_pointer": "$.data.users"
        }
    
    Fixed-width file:
        {
            "path": "/data/report.txt",
            "type": "fixed",
            "field_widths": [10, 30, 20],
            "field_names": ["id", "name", "email"],
            "has_header": false
        }
    """
    path: str = Field(..., description="Source file path")
    type: str = Field(..., description="File type: csv, tsv, json, jsonl, fixed")
    encoding: str = Field(default="utf-8")
    delimiter: Optional[str] = Field(default=",", description="For delimited files")
    has_header: bool = Field(default=True, description="For delimited files")
    skip_lines: int = Field(default=0, description="Lines to skip at start")
    # Fixed-width specific
    field_widths: Optional[List[int]] = Field(default=None, description="For fixed-width files")
    field_names: Optional[List[str]] = Field(default=None, description="Column names")
    # JSON specific
    records_pointer: Optional[str] = Field(default="$", description="JSONPath for JSON files")


class TargetFileConfig(BaseModel):
    """
    Target file configuration for writing.
    
    Supports multiple output formats:
    - CSV/TSV: Delimited output with headers
    - JSON: Pretty or compact JSON
    - JSONL: JSON Lines (streaming)
    - Fixed-Width: Formatted positional output
    
    Features:
    ---------
    - Gzip compression (.gz)
    - Custom encoding (UTF-8, Latin-1, etc.)
    - Header row option
    - Pretty print for JSON
    
    Examples:
    ---------
    CSV output:
        {
            "path": "/data/output.csv",
            "type": "csv",
            "header": true,
            "compress": false
        }
    
    Pretty JSON:
        {
            "path": "/data/output.json",
            "type": "json",
            "pretty": true
        }
    
    Fixed-width output:
        {
            "path": "/data/output.txt",
            "type": "fixed",
            "field_widths": [10, 30, 20],
            "align": "left",
            "strict_width": true
        }
    """
    path: str = Field(..., description="Target file path")
    type: str = Field(..., description="File type: csv, tsv, json, jsonl, fixed")
    encoding: str = Field(default="utf-8")
    delimiter: Optional[str] = Field(default=",")
    quotechar: Optional[str] = Field(default='"')
    header: bool = Field(default=True)
    compress: bool = Field(default=False, description="Gzip compression")
    # Fixed-width specific
    field_widths: Optional[List[int]] = Field(default=None)
    align: str = Field(default="left", description="left, right, center")
    pad_char: str = Field(default=" ")
    strict_width: bool = Field(default=False)
    # JSON specific
    pretty: bool = Field(default=False, description="Pretty print JSON")
    # Optional temporal fields (added to output if specified)
    asof_date: Optional[str] = Field(default=None, description="As-of date value (YYYY-MM-DD)")
    asof_date_column: Optional[str] = Field(default="asof_date", description="Column name for as-of date")
    correlation_id_column: Optional[str] = Field(default="correlation_id", description="Column name for correlation ID")


class FieldMapping(BaseModel):
    source: str
    target: str
    default: Optional[Any] = None


class ConversionRequest(BaseModel):
    """
    Main request model for /convert endpoint.
    
    Defines the complete ETL configuration for file-to-file conversion including:
    - Source file configuration
    - Target file configuration
    - Field mappings (optional)
    - Data validation schemas (Avro)
    - Data transformation rules (JOLT)
    - Data aggregation and grouping (NEW)
    
    Data Flow:
    ----------
    1. Read source file (CSV/JSON/JSONL/Fixed-Width)
    2. Validate against source_avro_schema
    3. Apply field mapping (if provided)
    4. Transform with JOLT (if provided)
    5. ðŸ†• Data aggregation and grouping (if provided)
    6. Validate against target_avro_schema
    7. Write target file (CSV/JSON/JSONL/Fixed-Width)
    8. Publish audit events to Kafka
    
    Example:
    --------
    See FILE_TO_FILE_SPECIFICATION.md for complete examples.
    """
    version: str = "1.0"
    source: SourceFileConfig
    target: TargetFileConfig
    mapping: Optional[List[FieldMapping]] = Field(default=None, description="Field mappings (optional)")
    jolt_spec: Optional[List[Dict[str, Any]]] = Field(default=None)
    aggregation_config: Optional[AggregationConfig] = Field(default=None, description="ðŸ†• Optional data aggregation and grouping configuration")
    source_avro_schema: Optional[Dict[str, Any]] = None
    target_avro_schema: Optional[Dict[str, Any]] = None
    batch_size: int = Field(default=10000, description="Processing batch size")


class ConversionResponse(BaseModel):
    """
    Response model for successful conversion operations.
    
    Returns summary statistics about the conversion operation including:
    - Status: "OK" for success
    - Total records converted
    - Source and target file paths
    - Total duration in milliseconds
    """
    status: str
    records_converted: int
    source_file: str
    target_file: str
    duration_ms: int


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================
# Note: Legacy file I/O functions have been replaced by shared processors:
# - FileReader (from common.processors.file_processor)
# - FileWriter (from common.processors.file_processor)
# - ETLPipeline (from common.processors.pipeline_processor)


async def audit_records_received(source: str, records_count: int, operation: str, correlation_id: str = None, **kwargs):
    """Publish audit event for records received from source."""
    if correlation_id is None:
        correlation_id = get_correlation_id()
    await publish_audit_event(
        domain_id=DEFAULT_DOMAIN_ID,
        entity_type="FILE",
        entity_id=source,
        event_type="RECORDS_RECEIVED",
        outcome="SUCCESS",
        details=f"Received {records_count} records from {source}",
        payload={"records_count": records_count, "operation": operation, "correlation_id": correlation_id, **kwargs},
        metadata={"source": source, "correlation_id": correlation_id},
    )


async def audit_records_processed(operation: str, records_count: int, duration_ms: float, source: str, destination: str, correlation_id: str = None, **kwargs):
    """Publish audit event for records processed."""
    if correlation_id is None:
        correlation_id = get_correlation_id()
    await publish_audit_event(
        domain_id=DEFAULT_DOMAIN_ID,
        entity_type="CONVERSION",
        entity_id=f"{source}->{destination}",
        event_type="RECORDS_PROCESSED",
        outcome="SUCCESS",
        details=f"Processed {records_count} records in {duration_ms:.2f}ms",
        payload={"records_count": records_count, "duration_ms": duration_ms, "operation": operation, "correlation_id": correlation_id, **kwargs},
        metadata={"source": source, "destination": destination, "correlation_id": correlation_id},
    )


async def audit_records_written(destination: str, records_count: int, duration_ms: float, correlation_id: str = None, **kwargs):
    """Publish audit event for records written to destination."""
    if correlation_id is None:
        correlation_id = get_correlation_id()
    await publish_audit_event(
        domain_id=DEFAULT_DOMAIN_ID,
        entity_type="FILE",
        entity_id=destination,
        event_type="RECORDS_WRITTEN",
        outcome="SUCCESS",
        details=f"Wrote {records_count} records to {destination} in {duration_ms:.2f}ms",
        payload={"records_count": records_count, "duration_ms": duration_ms, "correlation_id": correlation_id, **kwargs},
        metadata={"destination": destination, "correlation_id": correlation_id},
    )


from contextlib import asynccontextmanager

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup and shutdown events."""
    # Startup
    logger.info(f"Starting {SERVICE_NAME} service on port {os.getenv('PORT', '9093')}")
    logger.info(f"Log level: {LOG_LEVEL}, JSON logs: {JSON_LOGS}")
    logger.info(f"Security gateway: {os.getenv('SECURITY_BASE_URL', 'http://security-gateway:8088')}")
    
    if audit_publisher:
        await audit_publisher.start()
        logger.info("Audit publisher started")
    
    yield
    
    # Shutdown
    logger.info(f"Stopping {SERVICE_NAME} service")
    if audit_publisher:
        await audit_publisher.stop()
        logger.info("Audit publisher stopped")

# Update app with lifespan
app.router.lifespan_context = lifespan


# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/health")
async def health() -> Dict[str, str]:
    """
    Health check endpoint.
    
    Returns service health status for monitoring and load balancers.
    
    Returns:
        Dict with status="ok", service name, and correlation ID
        
    Example Response:
        {
            "status": "ok",
            "service": "file-to-file",
            "correlation_id": "health-check-123"
        }
    """
    logger.debug("Health check requested")
    return {"status": "ok", "service": SERVICE_NAME, "correlation_id": get_correlation_id()}

@app.get("/health", tags=["Healthcheck"])
def get_health_status():
    """    Health check endpoint to verify the API is running.
    Returns a simple status dictionary and an HTTP 200 OK.
    """
    return {"status": "healthy"}
    

@app.post("/convert", response_model=ConversionResponse)
@limiter.limit(STRICT_RATE_LIMIT)  # 10 requests per minute
async def convert_file(
    request: Request,  # Required by SlowAPI for rate limiting
    req: ConversionRequest,
    auth: Dict = Depends(verify_jwt_or_basic),
    x_correlation_id: str = Header(None)
) -> ConversionResponse:
    """
    Convert file from one format to another with full ETL pipeline.
    
    This is the main conversion endpoint that supports all format combinations:
    CSV â†” JSON â†” JSONL â†” TSV â†” Fixed-Width
    
    Authentication:
    ---------------
    Requires JWT token or Basic authentication (verified by security gateway).
    The authenticated user/subject is included in audit events.
    
    Data Flow:
    ----------
    1. **Read Source File**
       - CSV/TSV: Parse delimited data with headers
       - JSON: Parse JSON with JSONPath extraction
       - JSONL: Parse line-by-line JSON
       - Fixed-Width: Parse positional data
       - EDI: Parse with x12-edi-tools

    2. **Source Validation**
       - Validates data against source_avro_schema
       - Ensures data quality before transformation
    
    3. **Field Mapping (Optional)**
       - Maps source fields to target fields
       - Applies default values for nulls
    
    4. **Transformation (Optional)**
       - Applies JOLT transformations if jolt_spec provided
       - Supports shift, default, remove, modify operations
    
    5. **Target Validation**
       - Validates transformed data against target_avro_schema
       - Ensures schema compliance before file write
    
    6. **Write Target File**
       - CSV/TSV: Delimited output with headers
       - JSON: Pretty or compact JSON
       - JSONL: Streaming JSON Lines
       - Fixed-Width: Formatted positional output
       - Gzip compression (optional)
    
    7. **Audit Trail**
       - Publishes events to Kafka: records_received, records_processed,
         records_written
       - Full correlation ID tracking for data lineage
    
    Performance:
    ------------
    - Throughput: 20,000-100,000 records/sec depending on format
    - Memory: Loads entire file into memory (suitable for files up to 1GB)
    
    Args:
        request: ConversionRequest with complete configuration
        auth: Authentication context (injected by dependency)
    
    Returns:
        ConversionResponse with status, records_converted, file paths, and duration_ms
    
    Raises:
        HTTPException 400: Invalid request (unsupported format, file not found)
        HTTPException 500: Conversion failure (parse error, write error)
    
    Example:
        See FILE_TO_FILE_SPECIFICATION.md for complete request examples.
    """
    start_time = time.time()
    # correlation_id = get_correlation_id()
    correlation_id = x_correlation_id
    
    logger.info(
        f"Starting File-to-File conversion: {req.source.path} -> {req.target.path}",
        extra={
            "extra_fields": {
                "source_path": req.source.path,
                "source_type": req.source.type,
                "target_path": req.target.path,
                "target_type": req.target.type,
                "auth_user": auth.get("username") or auth.get("subject"),
                "has_jolt": req.jolt_spec is not None,
            }
        }
    )
    
    # Audit: Operation started
    await publish_audit_event(
        domain_id=DEFAULT_DOMAIN_ID,
        entity_type="OPERATION",
        entity_id=f"{req.source.path}->{req.target.path}",
        event_type="OPERATION_STARTED",
        outcome="INFO",
        details=f"Starting file-to-file conversion from {req.source.path} to {req.target.path}",
        payload={
            "operation_type": "file-to-file",
            "source": req.source.path,
            "destination": req.target.path,
            "correlation_id": correlation_id,
        },
        metadata={
            "correlation_id": correlation_id,
            "source": req.source.path,
            "destination": req.target.path,
        },
    )
    
    with OPERATION_LATENCY.labels(service=SERVICE_NAME, operation="convert").time():
        try:
            logger.debug(f"Reading source file: {req.source.path} (type: {req.source.type})")
            
            # Read source file using FileReader
            if req.source.type in ("csv", "tsv"):
                records = await FileReader.read_csv(
                    req.source.path,
                    encoding=req.source.encoding,
                    delimiter=req.source.delimiter,
                    has_header=req.source.has_header,
                    skip_lines=req.source.skip_lines,
                    field_names=req.source.field_names,
                )
            elif req.source.type == "json":
                records = await FileReader.read_json(
                    req.source.path,
                    encoding=req.source.encoding,
                    skip_lines=req.source.skip_lines,
                    records_pointer=req.source.records_pointer,
                )
            elif req.source.type == "jsonl":
                records = await FileReader.read_jsonl(
                    req.source.path,
                    encoding=req.source.encoding,
                    skip_lines=req.source.skip_lines,
                )
            elif req.source.type == "fixed":
                records = await FileReader.read_fixed_width(
                    req.source.path,
                    field_widths=req.source.field_widths,
                    field_names=req.source.field_names,
                    encoding=req.source.encoding,
                    has_header=req.source.has_header,
                    skip_lines=req.source.skip_lines,
                )

            else:
                raise HTTPException(
                    status_code=400,
                    detail=f"Unsupported source file type: {req.source.type}"
                )
            
            if not records:
                raise HTTPException(status_code=400, detail="No records found in source file")
            
            # Audit: Records received from source file
            await audit_records_received(
                source=req.source.path,
                records_count=len(records),
                operation="file_read",
                correlation_id=correlation_id,
                file_type=req.source.type,
            )
            
            logger.info(f"Read {len(records)} records from source file")
            
            # Process using ETLPipeline
            process_start = time.time()
            jolt_client = JoltClient() if req.jolt_spec else None
            mapping_dicts = None
            if req.mapping:
                mapping_dicts = [{"source": m.source, "target": m.target, "default": m.default} for m in req.mapping]
                field_names = [m.target for m in req.mapping]
            else:
                field_names = list(records[0].keys())

            if req.source_avro_schema:
                records = await ETLPipeline.process_batch(
                    records,
                    source_schema=req.source_avro_schema,
                    target_schema=None,  # Skip target validation until after temporal injection
                    jolt_spec=req.jolt_spec,
                    mapping=mapping_dicts,
                    jolt_client=jolt_client,
                    aggregation_config=req.aggregation_config,  # ðŸ†• Aggregation support
                )

            # Add optional temporal fields if specified
            if req.target.asof_date:
                correlation_id = get_correlation_id()
                for record in records:
                    record[req.target.asof_date_column] = req.target.asof_date
                    record[req.target.correlation_id_column] = correlation_id
                logger.info(f"Added temporal fields: {req.target.asof_date_column}={req.target.asof_date}, {req.target.correlation_id_column}={correlation_id}")
            
            # Validate against target Avro schema AFTER all processing (including aggregation and temporal injection)
            if req.target_avro_schema:
                from common.avro import validate_batch_list, normalize_records_for_schema
                # Normalize union types for target schema compatibility
                records = normalize_records_for_schema(records, req.target_avro_schema)
                # Validate final data structure against target schema
                validate_batch_list(records, req.target_avro_schema)
            
            # Audit: Records processed
            await audit_records_processed(
                operation="transform_validate",
                records_count=len(records),
                duration_ms=(time.time() - process_start) * 1000,
                source=req.source.path,
                destination=req.target.path,
                correlation_id=correlation_id,
                has_jolt=req.jolt_spec is not None,
            )
            
            logger.info(f"Processed {len(records)} records successfully")
            
            # Update field names from transformed data if JOLT was applied or aggregation was used
            if (req.jolt_spec or req.aggregation_config) and records:
                field_names = list(records[0].keys())
            
            # Write target file using FileWriter
            write_start = time.time()
            logger.debug(f"Writing target file: {req.target.path} (type: {req.target.type})")
            
            if req.target.type in ("csv", "tsv"):
                target_path = await FileWriter.write_csv(
                    records,
                    req.target.path,
                    field_names,
                    encoding=req.target.encoding,
                    delimiter=req.target.delimiter,
                    quotechar=req.target.quotechar,
                    header=req.target.header,
                    compress=req.target.compress,
                )
            elif req.target.type == "json":
                target_path = await FileWriter.write_json(
                    records,
                    req.target.path,
                    encoding=req.target.encoding,
                    pretty=req.target.pretty,
                    compress=req.target.compress,
                )
            elif req.target.type == "jsonl":
                target_path = await FileWriter.write_jsonl(
                    records,
                    req.target.path,
                    encoding=req.target.encoding,
                    compress=req.target.compress,
                )
            elif req.target.type == "fixed":
                target_path = await FileWriter.write_fixed_width(
                    records,
                    req.target.path,
                    field_names,
                    field_widths=req.target.field_widths,
                    encoding=req.target.encoding,
                    align=req.target.align,
                    pad_char=req.target.pad_char,
                    strict_width=req.target.strict_width,
                    header=req.target.header,
                    compress=req.target.compress,
                )
            else:
                raise HTTPException(
                    status_code=400,
                    detail=f"Unsupported target file type: {req.target.type}"
                )
            
            # Audit: Records written to target file
            await audit_records_written(
                destination=target_path,
                records_count=len(records),
                duration_ms=(time.time() - write_start) * 1000,
                correlation_id=correlation_id,
                operation="file_write",
                file_type=req.target.type,
            )
            
            # Metrics
            record_batch(SERVICE_NAME, "convert", len(records))
            record_success(SERVICE_NAME, "convert", len(records))
            duration_ms = int((time.time() - start_time) * 1000)
            
            logger.info(
                f"Conversion completed successfully: {len(records)} records in {duration_ms}ms",
                extra={
                    "extra_fields": {
                        "total_records": len(records),
                        "duration_ms": duration_ms,
                        "source_file": req.source.path,
                        "target_file": target_path,
                        "records_per_second": int(len(records) / (duration_ms / 1000)) if duration_ms > 0 else 0,
                    }
                }
            )
            
            # Audit: Operation completed successfully
            await publish_audit_event(
                domain_id=DEFAULT_DOMAIN_ID,
                entity_type="OPERATION",
                entity_id=f"{req.source.path}->{req.target.path}",
                event_type="OPERATION_COMPLETED",
                outcome="SUCCESS",
                details=f"Completed file-to-file conversion: {len(records)} records in {duration_ms:.2f}ms",
                payload={
                    "operation_type": "file-to-file",
                    "source": req.source.path,
                    "destination": target_path,
                    "records_count": len(records),
                    "duration_ms": duration_ms,
                    "correlation_id": correlation_id,
                },
                metadata={
                    "correlation_id": correlation_id,
                    "source": req.source.path,
                    "destination": target_path,
                },
            )
            
            return ConversionResponse(
                status="OK",
                records_converted=len(records),
                source_file=req.source.path,
                target_file=target_path,
                duration_ms=duration_ms,
            )
        
        except HTTPException:
            raise
        except Exception as e:
            duration_ms = (time.time() - start_time) * 1000
            
            # Audit: Operation failed
            await publish_audit_event(
                domain_id=DEFAULT_DOMAIN_ID,
                entity_type="OPERATION",
                entity_id=f"{req.source.path}->{req.target.path}",
                event_type="OPERATION_FAILED",
                outcome="FAILURE",
                details=f"File-to-file conversion failed: {type(e).__name__}: {str(e)}",
                payload={
                    "operation_type": "file-to-file",
                    "source": req.source.path,
                    "destination": req.target.path,
                    "error_type": type(e).__name__,
                    "error_message": str(e),
                    "duration_ms": duration_ms,
                    "correlation_id": correlation_id,
                },
                metadata={
                    "correlation_id": correlation_id,
                    "source": req.source.path,
                    "destination": req.target.path,
                },
            )
            
            log_error(logger, e, "convert",
                     source_path=req.source.path,
                     target_path=req.target.path)
            record_error(SERVICE_NAME, "convert", type(e).__name__)
            raise HTTPException(status_code=500, detail=f"Conversion failed: {str(e)}")


if __name__ == "__main__":
    """
    Run the File-to-File service directly (development mode).
    
    For production deployment, use:
    - Docker: See Dockerfile in service directory
    - Kubernetes: See deployment manifests in deploy/
    - Uvicorn: uvicorn app.file_to_file_service:app --host 0.0.0.0 --port 9093
    
    Environment Variables:
    ----------------------
    PORT: Service port (default: 9093)
    LOG_LEVEL: Logging level (default: INFO)
    JSON_LOGS: Enable JSON logging (default: false)
    SECURITY_BASE_URL: Security gateway URL (default: http://localhost:8088)
    KAFKA_BOOTSTRAP_SERVERS: Kafka servers for audit events
    KAFKA_AUDIT_TOPIC: Kafka topic for audit events (default: audit-events)
    AUDIT_ENABLED: Enable/disable audit publishing (default: true)
    """
    import uvicorn
    port = int(os.getenv("PORT", "9093"))
    uvicorn.run(app, host="0.0.0.0", port=port, log_level=LOG_LEVEL.lower())
